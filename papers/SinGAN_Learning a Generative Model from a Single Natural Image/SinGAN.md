# ■ 論文
- 論文タイトル："SinGAN: Learning a Generative Model from a Single Natural Image"
- 論文リンク：https://arxiv.org/abs/1905.01164
- 論文投稿日付：
- 被引用数（記事作成時点）：xxx 件
- 著者（組織）：
- categories：

# ■ 概要（何をしたか？）

## Abstract


# ■ イントロダクション（何をしたいか？）

## x. Introduction

- xxx

---

- Here, we take the use of GANs into a new realm – unconditional generation learned from a single natural image. Specifically, we show that the internal statistics of patches within a single natural image typically carry enough information for learning a powerful generative model. SinGAN, our new single image generative model, allows us to deal with general natural images that contain complex structures and textures, without the need to rely on the existence of a database of images from the same class. This is achieved by a pyramid of fully convolutional light-weight GANs, each is responsible for capturing the distribution of patches at a different scale. Once trained, SinGAN can produce diverse high quality image samples (of arbitrary dimensions), which semantically resemble the training image, yet contain new object configurations and structures1 (Fig. 1).
    - ここでは、GANの使用を新しい領域に取り入れます。これは、単一の自然画像から学んだ無条件の世代です。 具体的には、単一の自然画像内のパッチの内部統計には、通常、強力な生成モデルを学習するのに十分な情報が含まれていることを示しています。 新しい単一画像生成モデルであるSinGANを使用すると、同じクラスの画像のデータベースの存在に依存することなく、複雑な構造やテクスチャを含む一般的な自然画像を扱うことができます。 これは、完全に畳み込み式の軽量GANのピラミッドによって実現され、それぞれが異なるスケールでパッチの分布をキャプチャします。 トレーニングが完了すると、SinGANは多様な高品質の画像サンプル（任意の寸法）を生成できます。これは、トレーニング画像に意味的に似ていますが、新しいオブジェクトの構成と構造を含みます1（図1）

---

- xss

# ■ 結論

## x. Conclusion


# ■ 何をしたか？詳細

## 2. Method

- xxx

---

- We opt to go beyond texture generation, and to deal with more general natural images. This requires capturing the statistics of complex image structures at many different scales. For example, we want to capture global properties such as the arrangement and shape of large objects in the image (e.g. sky at the top, ground at the bottom), as well as fine details and texture information. To achieve that, our generative framework, illustrated in Fig. 4, consists of a hierarchy of patch-GANs (Markovian discriminator) [31, 26], where each is responsible for capturing the patch distribution at a different scale of x. The GANs have small receptive fields and limited capacity, preventing them from memorizing the single image. While similar multi-scale architectures have been explored in conventional GAN settings (e.g. [28, 52, 29, 52, 13, 24]), we are the first explore it for internal learning from a single image.
    - テクスチャ生成を超えて、より一般的な自然画像を扱うことを選択します。 これには、さまざまなスケールで複雑な画像構造の統計をキャプチャする必要があります。 たとえば、画像の大きなオブジェクト（上部の空、下部の地面など）の配置と形状などのグローバルプロパティ、および詳細とテクスチャ情報をキャプチャする必要があります。 これを実現するために、図4に示す生成フレームワークは、パッチGAN（Markovian discriminator）[31、26]の階層で構成され、それぞれが異なるスケールのパッチ分布をキャプチャする役割を果たします。 GANの受容フィールドは小さく、容量が限られているため、単一の画像を記憶できません。 同様のマルチスケールアーキテクチャは、従来のGAN設定（たとえば[28、52、29、52、13、24]）で検討されてきましたが、単一の画像から内部学習するための最初の調査です。

### 2.1. Multi-scale architecture

- xxx

---

- The generation of an image sample starts at the coarsest scale and sequentially passes through all generators up to the finest scale, with noise injected at every scale. All the generators and discriminators have the same receptive field and thus capture structures of decreasing size as we go up the generation process. At the coarsest scale, the generation is purely generative, i.e. GN maps spatial white Gaussian noise zN to an image sample x ̃N ,
    - 画像サンプルの生成は、最も粗いスケールで開始し、すべてのジェネレーターから最も細かいスケールまで順次通過し、すべてのスケールでノイズが注入されます。 すべてのジェネレーターとディスクリミネーターは同じ受容野を持ち、生成プロセスを進めるにつれてサイズが小さくなる構造をキャプチャします。 最も粗いスケールでは、生成は純粋に生成的です。つまり、GNは空間ホワイトガウスノイズzNを画像サンプルx ̃Nにマッピングします。

---

- The effective receptive field at this level is typically ∼ 1/2 of the image’s height, hence GN generates the general layout of the image and the objects’ global structure. Each of the generators G at finer scales (n < N) adds details that were not generated by the previous scales. Thus, in addition to spatial noise zn, each generator Gn accepts an upsampled version of the image from the coarser scale, i.e.,
    - このレベルでの有効な受容野は通常、画像の高さの約1/2であるため、GNは画像の一般的なレイアウトとオブジェクトのグローバル構造を生成します。 より細かいスケール（n <N）の各ジェネレーターGは、以前のスケールでは生成されなかった詳細を追加します。したがって、空間ノイズznに加えて、各ジェネレーターGnは、より粗いスケールからのアップサンプリングされたバージョンの画像を受け入れます。

---

- All the generators have a similar architecture, as depicted in Fig. 5. Specifically, the noise zn is added to the image (x ̃n+1 ) ↑r , prior to being fed into a sequence of convolutional layers. This ensures that the GAN does not disregard the noise, as often happens in conditional schemes involving randomness [62, 36, 63]. The role of the convonlutional layers is to generate the missing details in (x ̃n+1) ↑r (residual learning [22, 57]). Namely, Gn performs the operation
    - 図5に示すように、すべてのジェネレーターは同様のアーキテクチャを備えています。具体的には、畳み込み層のシーケンスに入力される前に、ノイズznが画像（x ̃n + 1）↑rに追加されます。 これにより、ランダム性を伴う条件付きスキームでよく発生するように、GANはノイズを無視しません[62、36、63]。 畳み込み層の役割は、（x ̃n + 1）↑rの欠落した詳細を生成することです（残差学習[22、57]）。 つまり、Gnは次の操作を実行します。


#### Reconstruction loss

- We want to ensure that there exists a specific set of input noise maps, which generates the original image x. We specifically choose {zrec,zrec ...,zrec} = {z∗,0,...,0}, where z∗ is some fixed noise map (drawn once and kept fixed during train- ing).
    - 元の画像xを生成する入力ノイズマップの特定のセットが存在することを確認する必要があります。 具体的には、{zrec、zrec ...、zrec} = {z ∗、0、...、0}を選択します。ここで、z ∗は何らかの固定ノイズマップです（一度描画され、トレーニング中に固定されたままです）。

- Denote by x ̃rec the generated image at the nth scale when using these noise maps. Then for n < N ,
    - これらのノイズマップを使用する場合、n番目のスケールで生成されたイメージをx ̃recで示します。 次に、n <Nの場合、

- The reconstructed image x ̃rec has another role during training, which is to determine the standard deviation σn of the noise zn in each scale. Specifically, we take σn to be proportional to the root mean squared error (RMSE) between (x ̃rec ) ↑r and x , which gives an indication of the amount of details that need to be added at that scale.
    - 再構成された画像x ̃recには、トレーニング中に別の役割があります。これは、各スケールのノイズznの標準偏差σnを決定することです。 具体的には、σnを（x ̃rec）↑rとxの間の二乗平均平方根誤差（RMSE）に比例させることで、そのスケールで追加する必要がある詳細の量を示します。



# ■ 実験結果（主張の証明）・議論（手法の良し悪し）・メソッド（実験方法）

## x. 論文の項目名

- Qualitative examples of our generated random image samples are shown in Fig. 1, Fig. 6, and many more examples are included in the SM. For each example, we show a number of random samples with the same aspect ratio as the original image, and with decreased and expanded dimensions in each axis. As can be seen, in all these cases, the generated samples depict new realistic structures and configuration of objects, while preserving the visual content of the training image. Our model successfully preservers global structure of objects, e.g. mountains (Fig. 1), air balloons or pyramids (Fig. 6), as well as fine texture information. Because the network has a limited receptive field (smaller than the entire image), it can generate new combinations of patches that do not exist in the training image Furthermore, we observe that in many cases reflections and shadows are realistically synthesized, as can be seen in Fig. 6 and Fig. 1 (and the first example of Fig. 8). Note that SinGAN’s architecture is resolution agnostic and can thus be used on high resolution images, as illustrated in Fig. 7 (see 4Mpix results in the SM). Here as well, structures at all scales are nicely generated, from the global arrangement of sky, clouds and mountains, to the fine textures of the snow.
    - 生成されたランダム画像サンプルの定性的な例を図1、図6に示し、さらに多くの例がSMに含まれています。各例について、元の画像と同じアスペクト比で、各軸の寸法を縮小および拡大したランダムサンプルをいくつか示します。ご覧のように、これらのすべてのケースで、生成されたサンプルは、トレーニング画像の視覚的な内容を維持しながら、オブジェクトの新しい現実的な構造と構成を示しています。私たちのモデルは、オブジェクトのグローバル構造を正常に保存します。山（図1）、気球またはピラミッド（図6）、細かいテクスチャ情報。ネットワークの受信フィールドは限られているため（画像全体よりも小さい）、トレーニング画像には存在しないパッチの新しい組み合わせを生成できます。さらに、多くの場合、反射と影が実際に合成されていることがわかります。図6および図1（および図8の最初の例）。 SinGANのアーキテクチャは解像度に依存しないため、図7に示すように高解像度の画像で使用できます（SMでの4Mpixの結果を参照）。ここでも、空、雲、山の全体的な配置から、雪のきめの細かいテクスチャまで、あらゆる規模の構造がうまく生成されています。
    
# ■ 関連研究（他の手法との違い）

## x. Related Work


