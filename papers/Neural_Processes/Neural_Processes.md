# ■ 論文
- 論文タイトル："Neural Processes"
- 論文リンク：https://arxiv.org/abs/1807.01622
- 論文投稿日付：2018/06/04
- 被引用数（記事作成時点）：xxx 件
- 著者（組織）：
- categories：

# ■ 概要（何をしたか？）

## Abstract

- A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.
    - ニューラルネットワーク（NN）は、勾配降下法で調整して、ラベル付きデータコレクションを高精度で近似できるパラメーター化された関数です。一方、ガウス過程（GP）は、可能な関数の分布を定義する確率モデルであり、確率推論のルールを介してデータに照らして更新されます。 GPは確率的で、データ効率が良く、柔軟性がありますが、計算量が多いため、適用範囲が制限されます。ニューラルプロセス（NP）と呼ばれる、両方の長所を組み合わせたニューラル潜在変数モデルのクラスを紹介します。 GPと同様に、NPは関数の分布を定義し、新しい観測に迅速に適応でき、予測の不確実性を推定できます。 NNと同様に、NPはトレーニングおよび評価中に計算効率が高くなりますが、データに優先順位を適合させることも学習します。回帰と最適化を含むさまざまな学習タスクでNPのパフォーマンスを実証し、文献の関連モデルと比較対照します。
    

# ■ イントロダクション（何をしたいか？）

## x. Introduction

- 第１パラグラフ

---

- 第２パラグラフ

# ■ 結論

## x. Conclusion


# ■ 何をしたか？詳細

## x. 論文の項目名


# ■ 実験結果（主張の証明）・議論（手法の良し悪し）・メソッド（実験方法）

## x. 論文の項目名


# ■ 関連研究（他の手法との違い）

## x. Related Work


