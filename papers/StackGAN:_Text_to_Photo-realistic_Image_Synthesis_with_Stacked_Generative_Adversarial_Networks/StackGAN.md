# ■ 論文
- 論文タイトル："StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"
- 論文リンク：https://arxiv.org/abs/1612.03242
- 論文投稿日付：2016/12/10
- 被引用数（記事作成時点）：xxx 件
- 著者（組織）：
- categories：

# ■ 概要（何をしたか？）

## Abstract

- Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts.
    - テキスト記述から高品質の画像を合成することは、コンピュータービジョンにおいて困難な問題であり、多くの実用的な用途があります。 既存のテキストからイメージへのアプローチによって生成されたサンプルは、与えられた説明の意味を大まかに反映することができますが、必要な詳細と鮮明な [vivid] オブジェクト部分を含んでいません。

- In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process.
    - 本論文では、テキスト記述を条件とする256×256の写実的な画像を生成するために、Stacked Generative Adversarial Networks（StackGAN）を提案します。 困難な問題を、スケッチの洗練プロセスを通じて、より管理しやすいサブ問題に分解します。

- The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. 
    - Stage-I GANは、指定されたテキスト記述に基づいてオブジェクトのプリミティブな形状と色をスケッチし、Stage-I低解像度画像を生成します。 Stage-II GANは、Stage-Iの結果とテキストの説明を入力として受け取り、写真のようにリアルな詳細を備えた高解像度の画像を生成します。 Stage-Iの結果の欠陥を修正し、洗練されたプロセスで説得力のある詳細を追加できます。 合成画像の多様性を改善し、条件付きGANのトレーニングを安定させるために、潜在的なコンディショニング多様体の滑らかさを促進する新しいコンディショニング増強技術を導入します。

- Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.
    - 大規模な実験とベンチマークデータセットの最新技術との比較により、提案された方法がテキスト記述を条件とするフォトリアリスティックな画像の生成で大幅な改善を達成することが実証されています。

# ■ イントロダクション（何をしたいか？）

## 1. Introduction

- Generating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc. Recently, Generative Adversarial Networks (GAN) [8, 5, 23] have shown promising results in synthesizing real-world images. Conditioned on given text descriptions, conditional-GANs [26, 24] are able to generate images that are highly related to the text meanings.
    - テキストから写真のようにリアルな画像を生成することは重要な問題であり、写真編集、コンピューター支援設計などの途方もない用途があります。最近、Generative Adversarial Networks（GAN）は実世界の画像の合成において有望な結果を示しています。 与えられたテキスト記述に基づいて、条件付きGAN [26、24]はテキストの意味に非常に関連する画像を生成することができます。

---

- However, it is very difficult to train GAN to generate high-resolution photo-realistic images from text descriptions. Simply adding more upsampling layers in state-of- the-art GAN models for generating high-resolution (e.g, 256×256) images generally results in training instability and produces nonsensical outputs (see Figure 1(c)). The main difficulty for generating high-resolution images by GANs is that supports of natural image distribution and implied model distribution may not overlap in high dimensional pixel space [31, 1]. This problem is more severe as the image resolution increases. Reed et al only succeeded in generating plausible 64×64 images conditioned on text descriptions [26], which usually lack details and vivid object parts, e.g, beaks and eyes of birds. Moreover, they were unable to synthesize higher resolution (e.g, 128×128) images without providing additional annotations of objects [24].
    - ただし、GANを訓練してテキスト記述から高解像度の写実的な画像を生成することは非常に困難です。 高解像度（256 x 256など）の画像を生成するために、最新のGANモデルにアップサンプリングレイヤーを追加するだけで、一般にトレーニングが不安定になり、無意味な出力が生成されます（図1（c）を参照）。 GANによる高解像度画像の生成の主な難点は、自然な画像分布と暗黙的なモデル分布のサポートが高次元のピクセル空間で重複しないことです[31、1]。 この問題は、画像の解像度が高くなるほど深刻です。 リード他は、テキストの説明[26]を条件とするもっともらしい64×64画像を生成することに成功しました。 さらに、オブジェクトの追加の注釈を提供せずに、より高い解像度（128 x 128など）の画像を合成することはできませんでした[24]。

---

- In analogy to how human painters draw, we decompose the problem of text to photo-realistic image synthesis into two more tractable sub-problems with Stacked Generative Adversarial Networks (StackGAN). Low-resolution images are first generated by our Stage-I GAN (see Figure 1(a)). On the top of our Stage-I GAN, we stack Stage-II GAN to generate realistic high-resolution (e.g, 256×256) images conditioned on Stage-I results and text descriptions (see Figure 1(b)). By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object. The support of model distribution generated from a roughly aligned low-resolution image has better probability of intersecting with the support of image distribution. This is the underlying reason why Stage-II GAN is able to generate better high-resolution images.
    - 人間の画家が描く方法と同様に [In analogy]、テキストの問題を写真のようにリアルな画像合成に分解 [decompose] し、Stacked Generative Adversarial Networks（StackGAN）を使用して、より扱いやすい [tractable] 2つの副問題に分解します。 低解像度の画像は、まずStage-I GANによって生成されます（図1（a）を参照）。 Stage-I GANの上に、Stage-II GANをスタックして、Stage-Iの結果とテキストの説明を条件とする現実的な高解像度（256 x 256など）の画像を生成します（図1（b）を参照）。 Stage-I GANは、Stage-Iの結果とテキストを再度条件付けすることで、Stage-I GANで省略されたテキスト情報のキャプチャを学習し、オブジェクトの詳細を描画します。 大まかに位置合わせされた低解像度画像から生成されたモデル分布のサポートは、画像分布のサポートと交差する確率が高くなります。 これが、Stage-II GANがより良い高解像度画像を生成できる根本的な理由です。

---

- In addition, for the text-to-image generation task, the limited number of training text-image pairs often results in sparsity in the text conditioning manifold and such sparsity makes it difficult to train GAN. Thus, we propose a novel Conditioning Augmentation technique to encourage smoothness in the latent conditioning manifold. It allows small random perturbations in the conditioning manifold and increases the diversity of synthesized images.
    - さらに、テキストから画像への生成タスクでは、限られた数のトレーニングテキストと画像のペアにより、多くの場合、テキスト条件付多様体にスパース性が生じ、そのようなスパース性によりGANのトレーニングが困難になります。 したがって、潜在的な条件多様体の滑らかさを促進するために、新しい条件付け拡張手法を提案します。 条件付け多様体で小さなランダムな摂動を可能にし、合成画像の多様性を高めます。

---

- The contribution of the proposed method is threefold: (1) We propose a novel Stacked Generative Adversarial Networks for synthesizing photo-realistic images from text descriptions. It decomposes the difficult problem of generating high-resolution images into more manageable subproblems and significantly improve the state of the art. The StackGAN for the first time generates images of 256×256 resolution with photo-realistic details from text descriptions. (2) A new Conditioning Augmentation technique is proposed to stabilize the conditional GAN training and also improves the diversity of the generated samples. (3) Extensive qualitative and quantitative experiments demonstrate the effectiveness of the overall model design as well as the effects of individual components, which provide useful information for designing future conditional GAN models. Our code is available at

# ■ 結論

## x. Conclusion


# ■ 何をしたか？詳細

## x. 論文の項目名


# ■ 実験結果（主張の証明）・議論（手法の良し悪し）・メソッド（実験方法）

## x. 論文の項目名


# ■ 関連研究（他の手法との違い）

## x. Related Work


