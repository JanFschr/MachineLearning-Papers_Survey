> 論文まとめ（要約ver）：https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/9

# ■ 論文
- 論文タイトル："StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks"
- 論文リンク：https://arxiv.org/abs/1612.03242
- 論文投稿日付：2016/12/10
- 被引用数（記事作成時点）：xxx 件
- 著者（組織）：
- categories：

# ■ 概要（何をしたか？）

## Abstract

- Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts.
    - テキスト記述から高品質の画像を合成することは、コンピュータービジョンにおいて困難な問題であり、多くの実用的な用途があります。 既存のテキストからイメージへのアプローチによって生成されたサンプルは、与えられた説明の意味を大まかに反映することができますが、必要な詳細と鮮明な [vivid] オブジェクト部分を含んでいません。

- In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process.
    - 本論文では、テキスト記述を条件とする256×256の写実的な画像を生成するために、Stacked Generative Adversarial Networks（StackGAN）を提案します。 困難な問題を、スケッチの洗練プロセスを通じて、より管理しやすいサブ問題に分解します。

- The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. 
    - Stage-I GANは、指定されたテキスト記述に基づいてオブジェクトのプリミティブな形状と色をスケッチし、Stage-I低解像度画像を生成します。 Stage-II GANは、Stage-Iの結果とテキストの説明を入力として受け取り、写真のようにリアルな詳細を備えた高解像度の画像を生成します。 Stage-Iの結果の欠陥を修正し、洗練されたプロセスで説得力のある詳細を追加できます。 合成画像の多様性を改善し、条件付きGANのトレーニングを安定させるために、潜在的なコンディショニング多様体の滑らかさを促進する新しいコンディショニング増強技術を導入します。

- Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.
    - 大規模な実験とベンチマークデータセットの最新技術との比較により、提案された方法がテキスト記述を条件とするフォトリアリスティックな画像の生成で大幅な改善を達成することが実証されています。

# ■ イントロダクション（何をしたいか？）

## 1. Introduction

- Generating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc. Recently, Generative Adversarial Networks (GAN) [8, 5, 23] have shown promising results in synthesizing real-world images. Conditioned on given text descriptions, conditional-GANs [26, 24] are able to generate images that are highly related to the text meanings.
    - テキストから写真のようにリアルな画像を生成することは重要な問題であり、写真編集、コンピューター支援設計などの途方もない用途があります。最近、Generative Adversarial Networks（GAN）は実世界の画像の合成において有望な結果を示しています。 与えられたテキスト記述に基づいて、条件付きGAN [26、24]はテキストの意味に非常に関連する画像を生成することができます。

---

- However, it is very difficult to train GAN to generate high-resolution photo-realistic images from text descriptions. Simply adding more upsampling layers in state-of- the-art GAN models for generating high-resolution (e.g, 256×256) images generally results in training instability and produces nonsensical outputs (see Figure 1(c)). The main difficulty for generating high-resolution images by GANs is that supports of natural image distribution and implied model distribution may not overlap in high dimensional pixel space [31, 1]. This problem is more severe as the image resolution increases. Reed et al only succeeded in generating plausible 64×64 images conditioned on text descriptions [26], which usually lack details and vivid object parts, e.g, beaks and eyes of birds. Moreover, they were unable to synthesize higher resolution (e.g, 128×128) images without providing additional annotations of objects [24].
    - ただし、GANを訓練してテキスト記述から高解像度の写実的な画像を生成することは非常に困難です。 高解像度（256 x 256など）の画像を生成するために、最新のGANモデルにアップサンプリングレイヤーを追加するだけで、一般にトレーニングが不安定になり、無意味な出力が生成されます（図1（c）を参照）。 GANによる高解像度画像の生成の主な難点は、自然な画像分布と暗黙的なモデル分布のサポートが高次元のピクセル空間で重複しないことです[31、1]。 この問題は、画像の解像度が高くなるほど深刻です。 リード他は、テキストの説明[26]を条件とするもっともらしい64×64画像を生成することに成功しました。 さらに、オブジェクトの追加の注釈を提供せずに、より高い解像度（128 x 128など）の画像を合成することはできませんでした[24]。

---

- In analogy to how human painters draw, we decompose the problem of text to photo-realistic image synthesis into two more tractable sub-problems with Stacked Generative Adversarial Networks (StackGAN). Low-resolution images are first generated by our Stage-I GAN (see Figure 1(a)). On the top of our Stage-I GAN, we stack Stage-II GAN to generate realistic high-resolution (e.g, 256×256) images conditioned on Stage-I results and text descriptions (see Figure 1(b)). By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object. The support of model distribution generated from a roughly aligned low-resolution image has better probability of intersecting with the support of image distribution. This is the underlying reason why Stage-II GAN is able to generate better high-resolution images.
    - 人間の画家が描く方法と同様に [In analogy]、テキストの問題を写真のようにリアルな画像合成に分解 [decompose] し、Stacked Generative Adversarial Networks（StackGAN）を使用して、より扱いやすい [tractable] 2つの副問題に分解します。 低解像度の画像は、まずStage-I GANによって生成されます（図1（a）を参照）。 Stage-I GANの上に、Stage-II GANをスタックして、Stage-Iの結果とテキストの説明を条件とする現実的な高解像度（256 x 256など）の画像を生成します（図1（b）を参照）。 Stage-I GANは、Stage-Iの結果とテキストを再度条件付けすることで、Stage-I GANで省略されたテキスト情報のキャプチャを学習し、オブジェクトの詳細を描画します。 大まかに位置合わせされた低解像度画像から生成されたモデル分布のサポートは、画像分布のサポートと交差する確率が高くなります。 これが、Stage-II GANがより良い高解像度画像を生成できる根本的な理由です。

---

- In addition, for the text-to-image generation task, the limited number of training text-image pairs often results in sparsity in the text conditioning manifold and such sparsity makes it difficult to train GAN. Thus, we propose a novel Conditioning Augmentation technique to encourage smoothness in the latent conditioning manifold. It allows small random perturbations in the conditioning manifold and increases the diversity of synthesized images.
    - さらに、テキストから画像への生成タスクでは、限られた数のトレーニングテキストと画像のペアにより、多くの場合、テキスト条件付多様体にスパース性が生じ、そのようなスパース性によりGANのトレーニングが困難になります。 したがって、潜在的な条件多様体の滑らかさを促進するために、新しい条件付け拡張手法を提案します。 条件付け多様体で小さなランダムな摂動を可能にし、合成画像の多様性を高めます。

---

- The contribution of the proposed method is threefold: (1) We propose a novel Stacked Generative Adversarial Networks for synthesizing photo-realistic images from text descriptions. It decomposes the difficult problem of generating high-resolution images into more manageable subproblems and significantly improve the state of the art. The StackGAN for the first time generates images of 256×256 resolution with photo-realistic details from text descriptions. (2) A new Conditioning Augmentation technique is proposed to stabilize the conditional GAN training and also improves the diversity of the generated samples. (3) Extensive qualitative and quantitative experiments demonstrate the effectiveness of the overall model design as well as the effects of individual components, which provide useful information for designing future conditional GAN models.
    - 提案された方法の貢献は3つあります。
    -（ 1）テキスト記述から写真のようにリアルな画像を合成するための、新規のスタック生成敵対ネットワークを提案します。 高解像度画像を生成するという困難な問題をより扱いやすいサブ問題に分解し、最新技術を大幅に改善します。 StackGANは、テキスト記述から写真のようにリアルな詳細を備えた256×256解像度の画像を初めて生成します。 
    - （2）条件付きGANトレーニングを安定化し、生成されたサンプルの多様性を改善するために、新しい条件付き拡張手法が提案されています。 
    - （3）広範な定性的および定量的実験は、モデル設計全体の有効性と個々のコンポーネントの効果を実証し、将来の条件付きGANモデルの設計に役立つ情報を提供します。

- Our code is available at https://github.com/hanzhanggit/StackGAN.

# ■ 結論

## x. Conclusion


# ■ 何をしたか？詳細

## 3. Stacked Generative Adversarial Networks

- xxx


### 3.2. Conditioning Augmentation

- As shown in Figure 2, the text description t is first encoded by an encoder, yielding a text embedding φt. In previous works [26, 24], the text embedding is nonlinearly transformed to generate conditioning latent variables as the input of the generator. However, latent space for the text embedding is usually high dimensional (> 100 dimensions). With limited amount of data, it usually causes discontinuity in the latent data manifold, which is not desirable for learning the generator.
    - 図2に示すように、テキスト記述tは最初にエンコーダーによってエンコードされ、φtを埋め込むテキストを生成します。 以前の作品[26、24]では、テキスト埋め込みは非線形に変換され、ジェネレーターの入力として条件付け潜在変数を生成します。 ただし、テキスト埋め込みの潜在スペースは通常、高次元（> 100次元）です。 データの量が限られている場合、通常、潜在データ多様体で不連続が発生しますが、これはジェネレーターの学習には望ましくありません。

- To mitigate this problem, we introduce a Conditioning Augmentation technique to produce additional conditioning variables cˆ. In contrast to the fixed conditioning text variable c in [26, 24], we randomly sample the latent variables cˆ from an independent Gaussian distribution N (μ(φt ), Σ(φt )), where the mean μ(φt ) and diagonal covariance matrix Σ(φt) are functions of the text embedding φt. The proposed Conditioning Augmentation yields more training pairs given a small number of image-text pairs, and thus encourages robustness to small perturbations along the conditioning manifold. To further enforce the smoothness over the conditioning manifold and avoid overfitting [6, 14], we add the following regularization term to the objective of the generator during training,
    - この問題を軽減するために、追加の調整変数cˆを生成するための調整強化手法を導入します。 [26、24]の固定条件付きテキスト変数cとは対照的に、独立したガウス分布N（μ（φt）、Σ（φt））から潜在変数cˆをランダムにサンプリングします。ここで、平均μ（φt）および対角 共分散行列Σ（φt）は、φtを埋め込むテキストの関数です。 提案されたコンディショニング拡張は、少数の画像とテキストのペアが与えられると、より多くのトレーニングペアを生成します。 コンディショニングマニホールドの平滑性をさらに強化し、オーバーフィッティング[6、14]を回避するために、トレーニング中にジェネレーターの目的に次の正則化用語を追加します。


- which is the Kullback-Leibler divergence (KL divergence) between the standard Gaussian distribution and the condi- tioning Gaussian distribution. The randomness introduced in the Conditioning Augmentation is beneficial for model- ing text to image translation as the same sentence usually corresponds to objects with various poses and appearances.
    - これは、標準ガウス分布と条件付きガウス分布の間のカルバック・ライブラー発散（KL発散）です。 コンディショニング拡張で導入されたランダム性は、同じ文が通常さまざまなポーズや外観を持つオブジェクトに対応するため、テキストから画像への翻訳のモデリングに役立ちます。

### 3.3. Stage-I GAN

- xxx

### 3.4. Stage-II GAN

- xxx

---

- Different from the original GAN formulation, the random noise z is not used in this stage with the assumption that the randomness has already been preserved by s0. Gaussian conditioning variables cˆ used in this stage and cˆ0 used in Stage-I GAN share the same pre-trained text encoder, generating the same text embedding φt. However, Stage- I and Stage-II Conditioning Augmentation have different fully connected layers for generating different means and standard deviations. In this way, Stage-II GAN learns to capture useful information in the text embedding that is omitted by Stage-I GAN.
    - 元のGAN定式化とは異なり、この段階ではランダムノイズzは使用されません。これは、ランダム性がs0によって既に保存されているという仮定によるものです。 このステージで使用されるガウス条件変数cˆとStage-I GANで使用されるcˆ0は同じ事前トレーニング済みテキストエンコーダーを共有し、φtを埋め込む同じテキストを生成します。 ただし、Stage-IとStage-IIの調整増強には、異なる平均と標準偏差を生成するための異なる完全に接続されたレイヤーがあります。 このようにして、Stage-II GANは、Stage-I GANで省略されているテキスト埋め込みの有用な情報をキャプチャすることを学習します。

# ■ 実験結果（主張の証明）・議論（手法の良し悪し）・メソッド（実験方法）

## x. 論文の項目名


# ■ 関連研究（他の手法との違い）

## x. Related Work


