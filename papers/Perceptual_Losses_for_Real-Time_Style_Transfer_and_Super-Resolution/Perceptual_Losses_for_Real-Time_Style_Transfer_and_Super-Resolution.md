# ■ 論文
- 論文タイトル："Perceptual Losses for Real-Time Style Transfer and Super-Resolution"
- 論文リンク：https://arxiv.org/abs/1603.08155
- 論文投稿日付：2016/03/27
- 著者（組織）：Justin Johnson, Alexandre Alahi, Li Fei-Fei
- categories：Style Transfer

# ■ 概要（何をしたか？）

## Abstract

- We consider image transformation problems, where an input image is transformed into an output image. 
    - 入力画像が出力画像に変換される画像変換の問題を考えます。

- Recent methods for such problems typically train feed-forward convolutional neural networks us- ing a per-pixel loss between the output and ground-truth images.
    - **そのような問題に対する最近の方法は、典型的には、出力画像とグランドトゥルース画像との間のピクセル毎の損失を用いてフィードフォワード畳み込みニューラルネットワークを訓練する。**

- Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features ex- tracted from pretrained networks.
    - **並行した研究は、事前に訓練されたネットワークから抽出された高水準の特徴に基づいて perceptual 損失関数を定義し最適化することによって高品質の画像が生成され得ることを示した。**

- We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks.
    - **我々は両方のアプローチの利点を結合し、そして画像変換タスクのためにフィードフォワードネットワークを訓練するための perceptual 損失関数の使用を提案する。**
    
- We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time.
    - 我々は、フィードフォワードネットワークがリアルタイムでGatysらによって提案された最適化問題を解決するために学習される画像のスタイル変換に関する結果を示す。

- Com- pared to the optimization-based method, our network gives similar qual- itative results but is three orders of magnitude faster.
    - **最適化ベースの方法と比較して、我々のネットワークは同様の定性的結果をもたらしますが、3桁速いです。**

- We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results. 
    - 我々はまた、単一画像の超解像化で実験し、そこでは、ピクセル当たりの損失を知覚的な損失で置き換えることは視覚的に好ましい結果を与える。


# ■ イントロダクション（何をしたいか？）

## x. 論文の項目名 (Introduction)

- 第１パラグラフ

---

- 第２パラグラフ

# ■ 結論

## x. 論文の項目名 (Conclusion)


# ■ 何をしたか？詳細

## 3 Method

![image](https://user-images.githubusercontent.com/25688193/59986755-0e7d2a00-9673-11e9-9202-7e97b8e8d5e3.png)

---

- As shown in Figure 2, our system consists of two components: an image transformation network $f_W$ and a loss network $\phi$ that is used to define several loss functions $l_1 , . . . , l_k$ .

- The image transformation network is a deep residual convolutional neural network parameterized by weights $W$ ; it transforms input images $x$ into output images $\hat{y}$ via the mapping $\hat{y} = f_W (x)$.

- Each loss function computes a scalar value $l_i(\hat{y}, y_i)$ measuring the difference between the output image $\hat{y}$ and a target image $y_i$.

- The image transformation network is trained using stochastic gradient descent to minimize a weighted combination of loss functions:

![image](https://user-images.githubusercontent.com/25688193/59986722-e5f53000-9672-11e9-958e-730fc222107f.png)

> 各層で計算された損失関数の期待値を最小化する image transform Net f の重み W を訓練によって得る。

---

- To address the shortcomings of per-pixel losses and allow our loss functions to better measure perceptual and semantic differences between images, we draw inspiration from recent work that generates images via optimization [6,7,8,9,10].
    - ピクセルごとの損失の欠点 [shortcomings] に対処し、我々の損失関数が画像間の知覚的および意味的な違いをより適切に測定できるようにするために、最適化によって画像を生成する最近の研究からインスピレーションを得ています[6,7,8,9,10]。

- The key insight of these methods is that convolutional neural networks pre-trained for image classification have already learned to encode the perceptual and semantic information we would like to measure in our loss functions.
    - これらの方法の重要な洞察は、画像分類のために予め訓練された畳み込みニューラルネットワークが、我々が損失関数において測定したい知覚的および意味的情報を符号化（＝エンコード）することをすでに学習しているということである。

- We therefore make use of a network φ which as been pretrained for image classification as a fixed loss network in order to define our loss functions.
    - それゆえ、我々は、我々の損失関数を定義するために、画像分類のために事前学習されたネットワーク φ を、固定された損失ネットワークとして利用する。

- Our deep convolutional transformation network is then trained using loss functions that are also deep convolutional networks.
    - そして、我々の深層畳み込み変換ネットワークは、同じく深層畳み込みネットワークでもある損失関数を使用して学習されます。

---

- The loss network $\phi$ is used to define a feature reconstruction loss $l_{feat}^\phi$ and a style reconstruction loss $l_{style}^\phi$ that measure differences in content and style between images.

- For each input image x we have a content target $y_c$ and a style target $y_s$.

![image](https://user-images.githubusercontent.com/25688193/59988477-24d8b500-9676-11e9-9050-2fda7b66265e.png)

- For style transfer, the content target $y_c$ is the input image $x$ and the output image $\hat{y}$ should combine the content of $x = y_c$ with the style of $y_s$; we train one network per style target.
    - スタイル変換の場合、目標コンテンツ $y_c$ は入力画像 $x$ と出力画像 $\hat{y}$ で、$x = y_c$ の内容と $y_s$ の画風を組み合わせます。
    - 目標スタイルごとに1つのネットワークを学習します。

- For single-image super-resolution, the input image x is a low-resolution input, the content target yc is the ground-truth high- resolution image, and the style reconstruction loss is not used; we train one network per super-resolution factor.
    - 単一画像の超解像化の場合、入力画像 $x$ は低解像度の入力であり、目標コンテンツ $y_c$ はグランドトゥルースの高解像度画像であり、スタイル再構成損失は使用されません。
    - 私達は超解像化因子ごとに一つのネットワークを訓練します。


### 3.1 Image Transformation Networks

- Our image transformation networks roughly follow the architectural guidelines set forth by Radford et al [42]. 

- We do not use any pooling layers, instead using strided and fractionally strided convolutions for in-network downsampling and upsampling.

- Our network body consists of five residual blocks [43] using the architecture of [44].

- All non-residual convolutional layers are followed by spatial batch normalization [45] and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that the output image has pixels in the range [0, 255].

- Other than the first and last layers which use 9 × 9 kernels, all convolutional layers use 3 × 3 kernels.

- The exact architectures of all our networks can be found in the supplementary material.
    - すべてのネットワークの正確なアーキテクチャは副教材 [supplementary material] に記載されています。


# ■ 実験結果（主張の証明）・議論（手法の良し悪し）・メソッド（実験方法）

## x. 論文の項目名


# ■ 関連研究（他の手法との違い）

## x. 論文の項目名（Related Work）


